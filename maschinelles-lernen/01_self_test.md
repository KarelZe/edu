# Selbsttestfragen

Nachfolgende Fragen eigenen sich zur PrÃ¼fungsvorbereitung mittels **Active Recall** gedacht und eine ErgÃ¤nzung zu Karteikarten.

Offizielle Fragen sind mit ğŸ§  markiert. Probeklausurfragen mit einem ğŸ¦§. Eigene Fragen haben keine Markierung.

## Linear Regression

* Under which assumptions is the least squares objective from linear regression equivalent to a maximum likelihood objective?ğŸ¦§

## Linear Classification

* 
## Model Selection

* Why is it a bad idea to evaluate your algorithm on the training set? ğŸ§ 
* What is the difference between true and empirical risk? ğŸ§ 
* The true risk can be decomposed in which parts?ğŸ§ 
* How is the bias and the variance of a learning algorithm defined and how do they contribute to the true risk?ğŸ§ 
* What is the advantage / disadvantage of k-fold CV vs. the Hold-out method?ğŸ§ 
* Why does it make sense to penalize the norm of the weight vector?ğŸ§ 
* Which norm can we use and what are the different effects?
* What is the effect of early stopping?ğŸ§ 

## Nearest Neighbour Algorithms, Trees and Forests

* What we mean  with non-parametric / instance-based machine learning algorithms?ğŸ§ 
* How $$k$$-Nearest neighbour works?ğŸ§ 
* Why is it hard to use for high dimensional data?ğŸ§ 
* How to search for nearest neighbours efficiently?ğŸ§ 
* What is a binary regression / decision tree?ğŸ§ 
* What are useful splitting criterions?ğŸ§ 
* How can we influence the model complexity of a tree?ğŸ§ 
* Why is it useful to use multiple trees and randomization?ğŸ§ 

## Clustering

* How is the clustering problem defined?ğŸ§ 
* Why is it called 'unsupervised'?ğŸ§ 
* How do hierarchical clustering methods work? ğŸ§ 
* What is the rule of the cluster-2-cluster distance and which distances can we use?ğŸ§ 
* How does the $$k$$-mean algorithm work? What are the two main steps?ğŸ§ 
* Why does the algorithm converge? What is it minimizing?ğŸ§ 
* Does $$k$$-means find a global minimum of the objective?ğŸ§ 

## Dimensionality Reduction

* What does dimensionality reduction mean?ğŸ§ 
* What is PCA? ğŸ§ 
* What are three things that it does?ğŸ§ 
* What are the roles of the Eigenvectors and Eigenvalues in PCA?ğŸ§ 
* Can you describe applications of PCA?ğŸ§ 

## Density Estimation and Expectation Maximization

* What are parametric methods and how to obtain their parameters?ğŸ§ 
* How many parameters have non-parametric methods?ğŸ§ 
* What are mixture models?ğŸ§ 
* Should gradient methods be used for training mixture models?ğŸ§ 
* How does the EM algorithm work?ğŸ§ 
* What is the biggest problem of mixture models?ğŸ§ 
* How does EM decomposes the marginal likelihood?ğŸ§ 
* Why does EM always improve the lower bound?ğŸ§ 
* Why does EM always improve the marginal likelihood?ğŸ§ 
* Why can we optimize each mixture component independently with EM?ğŸ§ 
* Why do we need sampling for continuous latent variables?ğŸ§ 

## Kernel methods

* What is the definition of a kernel and its relation to an underlying feature space?ğŸ§ 
* Why are kernels more powerful than traditional feature-based methods?ğŸ§ 
* What do we mean by the kernel trick?ğŸ§ 
* How do we apply the kernel trick to ridge regression?ğŸ§ 

## SVMs

* Why is it good to use a maximum margin objective for classification?ğŸ§ 
* How can we define the margin as an optimization problem?
* What are slack variables and how can they be used to get a 'soft' margin?ğŸ§ 
* How is the hinge loss defined?ğŸ§ 
* What is the relation between the slack variables and the hinge loss?ğŸ§ 
* What are advantages and disadvantages in comparison to logistic regression?ğŸ§ 
* What is the difference between gradients and sub-gradients?ğŸ§ 

## Bayesian Learning

* What are the 2 basic steps behind Bayesian Learning?ğŸ§ 
* Why is Bayesian Learning more robust against overfitting?ğŸ§ 
* What happens with the posterior if we add more data to the training set?ğŸ§ 
* What is completing the square and how does it work?ğŸ§ 
* For which 2 cases can Bayesian Learning be solved in closed form?ğŸ§ 
* Which approximations can we use if no closed form is available?
* How can we derive Bayesian Linear regression?ğŸ§ 
* What is the advantage of Bayesian Linear Regression over Ridge regression? What is the conceptual difference?ğŸ§ 
* What is the major advantage of Gaussian processes over kernel ridge regression?ğŸ§ 
* Why are GPs a Bayesian approach?ğŸ§ 
* What principle allowed deriving GPs from a Bayesian regression point of view?ğŸ§ 

## Neural Nets

* How odes logistic regression relate to neural networks?ğŸ§ 
* What kind of functions can single layer neural networks learn?ğŸ§ 
* Why do we need non-linear activation functions?ğŸ§ 
* What activation functions can we use and what are the advantages / disadvantages of those?ğŸ§ 
* What output layer and loss function to use given the task \(regression, classification\)?ğŸ§ 
* Why not use a sigmoid activation function?ğŸ§ 
* Derive the equations for forward and backpropagation for a simple network.ğŸ§ 
* What is mini-batch gradient descent? Why use it instead of stochastic gradient descent or full gradient descent?ğŸ§ 
* Why neural networks can overfit and what are the options to prevent it?ğŸ§ 
* Why is the initialization of the network important?ğŸ§ 
* What can you read from the loss-curves during training \(validation and training loss\)?ğŸ§ 
* How can we accelerate gradient descent?ğŸ§ 
* How does Adam work?ğŸ§ 

## CNNs

* Why are fully connected networks for images a bad idea and why do we need images?ğŸ§ 
* What are the key components of a CNN?ğŸ§ 
* What hyper-parameters can we set for a convolutional layer and what is their meaning?ğŸ§ 
* What hyper-parameters can we set for a pooling layer and what is their meaning?ğŸ§ 
* How can we compute dimensionality of the output of a convolutional layer?ğŸ§ 
* Describe basic properties of 'AlexNet' and 'VCG'.ğŸ§ 
* What is the main idea of 'ResNet' to make it very deep?ğŸ§ 

