# Selbsttestfragen

Nachfolgende Fragen eigenen sich zur PrÃ¼fungsvorbereitung mittels **Active Recall** gedacht und eine ErgÃ¤nzung zu Karteikarten.

Offizielle Fragen sind mit ğŸ§  markiert. Probeklausurfragen mit einem ğŸ¦§. Fragen der University of Berkley mit ğŸ§‘â€ğŸš’. Eigene Fragen haben keine Markierung.

## Linear Regression

* Under which assumptions is the least squares objective from linear regression equivalent to a maximum likelihood objective?ğŸ¦§
* Give the formula for ridge regression and explain its components.
* Give the formula for LASSO and explain its components.
* Compare Multiple Linear Regression to LASSO. Why is it desirable to penalize a Linear Regression model?
* Why do errors of a linear regression model have to be normally distributed?

## Linear Classification

* Write down the objective of linear binary logistic regression. The samples are given by $$x_{i}$$ and the labels by $$c_{i} \in\{0,1\}$$. How is $$p\left(c_{i} \mid \boldsymbol{x}_{i}\right)$$ assumed to be distributed in binary logistic regression?ğŸ¦§

## Model Selection

* Why is it a bad idea to evaluate your algorithm on the training set? ğŸ§ 
* What is the difference between true and empirical risk? ğŸ§ 
* The true risk can be decomposed in which parts?ğŸ§ 
* How is the bias and the variance of a learning algorithm defined and how do they contribute to the true risk?ğŸ§ 
* What is the advantage/disadvantage of k-fold CV vs. the Hold-out method?ğŸ§ 
* Why does it make sense to penalize the norm of the weight vector?ğŸ§ 
* Which norm can we use and what are the different effects?
* What is the effect of early stopping?ğŸ§ 

## Nearest Neighbour Algorithms, Trees, and Forests

* What we mean with non-parametric / instance-based machine learning algorithms?ğŸ§ 
* How $$k$$-Nearest neighbour works?ğŸ§ 
* Why is it hard to use for high-dimensional data?ğŸ§ 
* How to search for nearest neighbours efficiently?ğŸ§ 
* What is a binary regression / decision tree?ğŸ§ 
* What are useful splitting criterions?ğŸ§ 
* How can we influence the model complexity of a tree?ğŸ§ 
* Why is it useful to use multiple trees and randomization?ğŸ§ 
* Name at least two advantages and two disadvantages of decision trees. ğŸ¦§
* Which data structure is usually used to efficiently implement k-Nearest Neighbours? Name the main steps in building that data structure.ğŸ¦§

## Clustering

* _How is the clustering problem defined?ğŸ§ _
  * Clustering tries to find a natural grouping within the data.
  * One tries to maximize the intra-cluster similarity and minimize the inter-cluster similarity.
* _Why is it called 'unsupervised'?ğŸ§ _
  * It's called unsupervised, as the data is unlabeled. There is no supervisor, that tells us in advance to which cluster a training data point belongs. Hence, unsupervised. The structure is actually learned from the data. 
* _How do hierarchical clustering methods work?_ ğŸ§ 
  * **Init-phase:** Each of the $$n$$samples becomes a cluster itself
  * **Iterate-phase:** 
    1. Find closest clusters and merge them
    2. Proceed until we have a single cluster
* _What is the rule of the cluster-2-cluster distance and which distances can we use?_ğŸ§ 
  * So called cluster linkage types define the distance between two clusters. Commonly used are
    * Single linkage
    * Complete linkage
    * Average linkage
    * Centroid linkage
* _How does the_ $$k$$_-mean algorithm work? What are the two main steps?_ğŸ§ 
  * The two main steps are the **assigment** and **adjustment step**.
  * **Assignment step:** 
    * Assign each sample to its closest centroid $$z_{n}=\arg \min_{k}\left\|\boldsymbol{c}_{k}-\boldsymbol{x}_{n}\right\|^{2}$$ 
  * **Adjustment Step:**
    * Adjust the centroids to be the means of the samples assigned to them: $$c_{k}=\frac{1}{\left|X_{k}\right|} \sum_{\boldsymbol{x}_{i} \in X_{k}} \boldsymbol{x}_{i}, \quad X_{k}=\left\{\boldsymbol{x}_{n} \mid z_{n}=k\right\}$$ 
* _Why does the algorithm converge? What is it minimizing?_ğŸ§ 
  * $$k$$-means minimizes the Sum of Squared Distances \(SSD\). By checking, if a cluster centeroids exists, that is closer to a point than its current cluster centroid the SSD is reduced. If no closer custer centre exists and the SSD remains constant.
  * SSD is defined as $$\operatorname{SSD}(C ; \mathcal{D})=\sum_{i=1}^{n} d\left(\boldsymbol{x}_{i}, c\left(\boldsymbol{x}_{i}\right)\right)^{2} $$ 
* _Does_ $$k$$_-means find a global minimum of the objective?ğŸ§ _
  * No, generally it doesn't. Finding a global minimum is a NP-hard problem. One would have to check all assignments to find the global best solution.
  * More over, the result of $$k$$-means is heavily dependent on the initialisation.

## Dimensionality Reduction

* What does dimensionality reduction mean?ğŸ§ 
* What is PCA? ğŸ§ 
* What are three things that it does?ğŸ§ 
* What are the roles of the Eigenvectors and Eigenvalues in PCA?ğŸ§ 
* Can you describe applications of PCA?ğŸ§ 
* Why should each dimension have a unit variance / be normalized?

## Density Estimation and Expectation Maximization

* What are parametric methods and how to obtain their parameters?ğŸ§ 
* How many parameters have non-parametric methods?ğŸ§ 
* What are mixture models?ğŸ§ 
* Should gradient methods be used for training mixture models?ğŸ§ 
* How does the EM algorithm work?ğŸ§ 
* What is the biggest problem of mixture models?ğŸ§ 
* How does EM decomposes the marginal likelihood?ğŸ§ 
* Why does EM always improve the lower bound?ğŸ§ 
* Why does EM always improve the marginal likelihood?ğŸ§ 
* Why can we optimize each mixture component independently with EM?ğŸ§ 
* Why do we need sampling for continuous latent variables?ğŸ§ 

## Kernel methods

* What is the definition of a kernel and its relation to an underlying feature space?ğŸ§ 
* Why are kernels more powerful than traditional feature-based methods?ğŸ§ 
* What do we mean by the kernel trick?ğŸ§ 
* How do we apply the kernel trick to ridge regression?ğŸ§ 
* Study the kernel properties, where is symmetry relevant? Where and why is positive definiteness of matrix important?
* What is the impact of changing $$\sigma$$ in a RBF kernel?
* What is the actual gain from using polynomial kernels?

## SVMs

* Why is it good to use a maximum margin objective for classification?ğŸ§ 
* How can we define the margin as an optimization problem?
* What are slack variables and how can they be used to get a 'soft' margin?ğŸ§ 
* How is the hinge loss defined?ğŸ§ 
* What is the relation between the slack variables and the hinge loss?ğŸ§ 
* What are advantages and disadvantages in comparison to logistic regression?ğŸ§ 
* What is the difference between gradients and sub-gradients?ğŸ§ 
* First, explain the intuition behind slack-variables in support vector machine training. Second, for a single data-point $$\left(\boldsymbol{x}_{i}, c_{i}\right)$$ the margin condition with slack variable $$\xi_{i}$$ is given as

  $$
  c_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1-\xi_{i}
  $$

  * Assuming $$0 \leq \xi_{i} \leq 1$$, is $$\boldsymbol{x}_{i}$$ classified correctly?
  * Assuming $$\xi_{i}>1$$, is $$x_{i}$$ classified correctly?ğŸ¦§

## Bayesian Learning

* What are the two basic steps behind Bayesian Learning?ğŸ§ 
* Why is Bayesian Learning more robust against overfitting?ğŸ§ 
* What happens with the posterior if we add more data to the training set?ğŸ§ 
* What is completing the square and how does it work?ğŸ§ 
* For which 2 cases can Bayesian Learning be solved in closed form?ğŸ§ 
* Which approximations can we use if no closed form is available?
* How can we derive Bayesian Linear regression?ğŸ§ 
* What is the advantage of Bayesian Linear Regression over Ridge regression? What is the conceptual difference?ğŸ§ 
* What is the major advantage of Gaussian processes over kernel ridge regression?ğŸ§ 
* Why are GPs a Bayesian approach?ğŸ§ 
* What principle allowed deriving GPs from a Bayesian regression point of view?ğŸ§ 
* Gaussian Processes\(GP\) are also referred to as a "Bayesian Kernel Regression" approach. Why? ğŸ¦§

## Neural Nets

* How does logistic regression relate to neural networks?ğŸ§ 
* What kind of functions can single-layer neural networks learn?ğŸ§ 
* Why do we need non-linear activation functions?ğŸ§ 
* What activation functions can we use and what are the advantages/disadvantages of those?ğŸ§ 
* What output layer and loss function to use given the task \(regression, classification\)?ğŸ§ 
* Why not use a sigmoid activation function?ğŸ§ 
* Derive the equations for forward and backpropagation for a simple network.ğŸ§ 
* What is mini-batch gradient descent? Why use it instead of stochastic gradient descent or full gradient descent?ğŸ§ 
* Why neural networks can overfit and what are the options to prevent it?ğŸ§ 
* Why is the initialization of the network important?ğŸ§ 
* What can you read from the loss-curves during training \(validation and training loss\)?ğŸ§ 
* How can we accelerate gradient descent?ğŸ§ 
* How does Adam work?ğŸ§ 
* What is the key idea behind second-order optimization methods? What are their benefits? Why are second-order optimization methods usually not applicable for Deep Neural Networks? ğŸ¦§
* _Explain why sigmoid activation results tend to be almost_ $$0$$ _or_ $$1$$?
* Adding more hidden layers will solve the vanishing gradient problem for a two-layer neural network. ğŸ§‘â€ğŸš’
* xxx suffer\(s\) from the vanishing gradient problem. Circle all that apply and JUSTIFY YOUR ANSWER. ğŸ§‘â€ğŸš’
* Adding L2-regularization will help with vanishing gradients ğŸ§‘â€ğŸš’
* Early stopping, cross-validation, and network pruning are techniques to prevent overfitting of Neural Nets. Explain them. 
* For a fully-connected deep network with one hidden layer, increasing the number of hidden units should have what effect on bias and variance? ğŸ§‘â€ğŸš’
* What is the problem of Neural nets with many parameters?
* Explain what network pruning is.
* Explain how early stopping works with Neural Nets.

## CNNs

* Why are fully connected networks for images a bad idea and why do we need images?ğŸ§ 
* What are the key components of a CNN?ğŸ§ 
* What hyper-parameters can we set for a convolutional layer and what is their meaning?ğŸ§ 
* What hyper-parameters can we set for a pooling layer and what is their meaning?ğŸ§ 
* How can we compute dimensionality of the output of a convolutional layer?ğŸ§ 
* Describe basic properties of 'AlexNet' and 'VCG'.ğŸ§ 
* What is the main idea of 'ResNet' to make it very deep?ğŸ§ 
* Why is it not feasible to use a fully connected layer for images? How do convolutional neural networks solve this problem and which property of an image do they exploit?ğŸ¦§
* How do LSTMs actually avoid the vanishing gradient problem?

## General

* What is the difference between AI and ML?
* What are the hyperparameters for choosing the model complexity for each of the following algorithms. Name at least one hyperparameter for every algorithm.
  * Neural Networks
  * Support Vector Machines
  * Gaussian Processes
  * Decision Trees? ğŸ¦§
* You are given the following optimization problem:

  $$
  \begin{aligned}
  &\underset{a}{\operatorname{argmax}} a^{2} h \\
  &\qquad \text { s.t. } S_{\max } \geq 2 a^{2}+4 a h
  \end{aligned}
  $$

  Write down the Lagrangian. Derive the optimal value for $$a$$ depending on your lagrangian multiplier. ğŸ¦§

